线性回归：
优点：结果易于理解，计算上不复杂
缺点：对非线性数据拟合不好
使用数据类型：数值型和标称型数据

回归的一般方法：
1、准备数据：回归需要数值型数据，标称型数据将被转化为二值型数据
2、分析数据：绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘制在图上作为对比
3、训练算法：找到回归系数
4、测试算法：找到R^2或者预测值和数据的拟合度，来分析模型的效果
5、使用算法：使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签

在使用ex0的数据进行回归分析的时候，局部线性加权参数的选择最优为0.01；而在预测鲍鱼年龄的时候，最佳选择是0.1；对于不同的数据集，参数选择是不同的。
而且虽然都是很小的值，但是不能都选择很小的值，因为值过小容易产生过拟合现象

线性回归的一个问题是有可能出现欠拟合现象，因为它求得是具有最小均方误差的无偏估计。使用局部线性加权，我们给待预测点附近的每个点赋予一定的权重，在这个子集
上基于最小均方差进行回归。使用不同的核进行对附近点赋予更高的权重。最常用的核实高斯核

局部线性加权的问题：每次必须在整个数据集上运行，即为了做出预测，必须保存所有的训练数据。

当数据的特征数比样本点数还要多的时候，不能使用线性回归。为解决此问题，使用岭回归（ridge regression）。
简单来说，岭回归就是在xTx上加上一个rI使得矩阵非奇异，进而能够对xTx + lamdba I求逆。
岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计，此处用过引入lamdba来限制所有的w之和，
通过引入该惩罚项，能够减少不重要的参数，这种技术为缩减。
此处通过误差最小化来获取lamdba，获取数据后，首先抽取一部分数据用于测试，剩余的作为训练集用于训练参数w。训练完毕后在测试集上测试预测性能。
通过选取不同的lamdba来重复上述预测过程，最终得到一个使预测误差最小的lamdba.

前向逐步回归的思想：一开始，所有权重都设为1，然后每一步所做出的决策是对某个权重增加或减少一个很少的值
步骤：
数据标准化，使其分布满足0均值和单位方差
在每轮迭代过程中：
    设置当前最小误差lowestError为正无穷
    对每个特征：
        增大或减小：
            改变一个系数得到一个新的W
            计算新W下的误差
            如果误差Error小于当前最小误差lowestError：设置Wbest等于当前的W
        将W设置为新的Wbest




