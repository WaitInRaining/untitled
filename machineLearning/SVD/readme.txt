奇异值分解：
优点：简化数据、去除噪声、提高算法的结果
缺点：数据的转换难以理解


奇异值分解的几何意义：对于任何一个矩阵，我们要找到一组两两正交单位向量序列，使得矩阵作用在此向量序列上后得到新的向量序列
保持两两正交。奇异值的几何含义：这组变换后的新的向量序列的长度。
奇异值分解会将矩阵分解为三个矩阵：U、A、Vt（V的转置）
如果原始矩阵DATA为m* n矩阵，那么U为m*m矩阵，A为m*n矩阵，Vt为n*n矩阵。其中A为对角矩阵，只有对角元素，其他元素全为0，
且对角元素从大到小排列。这些对角元素就是奇异值。这些奇异值 就是矩阵Data*Data.T特征值的平方根。
在某个奇异值的数据（r个）之后，其他的奇异值都置为0，这就意味着数据集中仅有r个重要的特征，而其他特征则是噪声或者冗余特征。

奇异值越大，代表含有的信息越多。确定要保留的奇异值的数目有很多方法，一个典型的做法就是保留矩阵中90%的能量信息。
为计算能量信息，我们将所有的奇异值求其平方和。可以将奇异值的平方和累加到总值的90%为止。
另一种启发式策略为：当奇异值有上万个的时候，就保留前面2000或者3000个，但是这样并不能保证包含90%有用的信息。

基于物品的相似度计算的时间会随着物品数量的增加而增加
基于用户的相似度计算的时间会随着用户数量的增加而增加



