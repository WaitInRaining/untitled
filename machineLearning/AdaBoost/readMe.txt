使用集成算法有多种形式：
1、不同算法的继承
2、同一算法在不同设置下的集成
3、数据集不同部分分配给不同分类器之后的继承

AdaBoost：
优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整
缺点：对离群点敏感
适用类型：数值型和标称型


自举汇聚法（booststrap aggergating）bagging：
在原始数据集中选择S次后，得到S个新数据集的一种技术。新数据集和原数据集的大小相同，是从原数据集中进行有放回随机抽取得来的。
因此允许新数据集中样本存在重复。
在S个新数据集建立完成后，将某个学习算法分别应用到每个数据集得到S个分类器。当我们对一个新的样本进行分类时，就使用者S个分类器进行分类，然后进行投票选取分类结果。
Boost：
在Boost中，不同的分类器是通过串行训练而得到的，每个新分类器根据已训练处的分类器的性能进行训练。
Boost通过集中关注已被分类器错分的样本来获得新的分类器。
Boost分类的结果是基于所有分类器的加权求和结果的，因此boosting中分类器的权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。

AdaBoost的一般流程：
1、准备数据：依赖于所使用的弱分类器类型，本次使用单层决策树，这种分类器可以处理任何数据类型。作为弱分类器，简单分类器的效果更好
2、训练数据：AdaBoost的大部分时间在训练上，分类器将多次在同一数据集上训练弱分类器
3、测试算法：测试算法的错误率
4、使用算法：同SVM一样，AdaBoost预测两个类别中的一个，如果想把它应用到多个类别的场合，需要对AdaBoost进行修改。

AdaBoost运行过程：训练数据中的每个样本，并赋予其一个权重，这些权重构成一个向量D。这些权重初始化为相同值。
首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后再统一数据集上在此训练弱分类器。
在分类器的二次训练中，将重新调整每个样本的权重，其中第一次分对的样本权重将会降低，而分错的样本的权重将会提升。
为了从所有弱分类器中得到最终的分类器结果，AdaBoost为每个分类器都分配了一个权重值alpha，这些alpha值是基于每个弱分类器的错误率计算得到的
alpha = ln((1-errRate) / errRate) / 2
计算出alpha值后，可以对权重向量D进行更新，以使得那些正确分类的样本的权重降低，错误分类样本的权重升高。
如果一个样本被正确分类，其权重改变DI(next) = DI(pre)e^(-alpha) / sum(D)
如果一个样本被错误分类，其权重改变DI（next） = DI（pre）e^(alpha) / sum(D)
得到D后，进行下一次的迭代，知道训练的错误率为0或者弱分类器的数目达到用户的指定值为止。


基于单层决策树构建弱分类器的伪代码：
将最小错误minError设为正无穷大
对数据集中的每一个特征（第一层循环）:
    对每个补偿（第二层循环）:
        对每个不等号（第三层循环）：
            建立一棵单层决策树并利用加权数据对它进行测试
               如果错误率低于minError，则将当前单层决策树设为最佳单层决策树
返回最佳单层决策树

使用单层决策树建立AdaBoost：
对于每次迭代：
    利用buildStump（）函数找到最佳的单层决策树
    将单层决策数加入到单层决策树组
    计算alpha值
    计算新的权重向量D
    更新累计类别估计值
    如果错误率为0 ，退出循环
#错误率为0这个退出条件并不好，一般情况是，错误变化率变化不大的时候退出，即趋于稳定后退出


非均衡分类：
二分类问题的混淆矩阵： TP、FN、FP、TN
precision = TP /(TP +FP)
recall = TP / (TP + FN)
ROC曲线，以TP/(FP + TN)为横坐标，以TP/(TP+FN)为纵坐标，绘制的曲线。




